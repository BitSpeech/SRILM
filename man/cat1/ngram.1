ngram(1)                                                 ngram(1)



NNAAMMEE
       ngram - apply N-gram language models

SSYYNNOOPPSSIISS
       nnggrraamm [--hheellpp] option ...

DDEESSCCRRIIPPTTIIOONN
       nnggrraamm  performs  various  operations with N-gram-based and
       related language models, including sentence scoring,  per-
       plexity  computation,  sentences  generation,  and various
       types of model interpolation.  The N-gram language  models
       are  read  from  files  in  ARPA  nnggrraamm--ffoorrmmaatt(5); various
       extended language model formats  are  described  with  the
       options below.

OOPPTTIIOONNSS
       Each  filename  argument  can  be an ASCII file, or a com-
       pressed file (name ending in .Z or .gz), or ``-'' to indi-
       cate stdin/stdout.

       --hheellpp  Print option summary.

       --vveerrssiioonn
              Print version information.

       --oorrddeerr _n
              Set the maximal N-gram order to be used, by default
              3.  NOTE: The order of the model is not  set  auto-
              matically  when  a  model file is read, so the same
              file can be used at various orders.  To use  models
              of  order  higher  than 3 it is always necessary to
              specify this option.

       --ddeebbuugg _l_e_v_e_l
              Set the debugging output level (0 means  no  debug-
              ging  output).   Debugging  messages  are  sent  to
              stderr,  with  the  exception  of  --ppppll  output  as
              explained below.

       --mmeemmuussee
              Print memory usage statistics for the LM.

       The following options determine the type of LM to be used.

       --nnuullll  Use a `null' LM as the main model (one  that  gives
              probability  1  to  all  words).  This is useful in
              combination with mixture creation or for debugging.

       --llmm _f_i_l_e
              Read  the  (main)  N-gram  model  from  _f_i_l_e.  This
              option is always required, unless --nnuullll was chosen.

       --ttaaggggeedd
              Interpret the LM as containing word/tag N-grams.

       --sskkiipp  Interpret the LM as a ``skip'' N-gram model.

       --hhiiddddeenn--vvooccaabb _f_i_l_e
              Interpret the LM as an N-gram model containing hid-
              den events between words.  The list of hidden event
              tags is read from _f_i_l_e.
              Hidden event definitions may also follow the N-gram
              definitions in the LM file (the argument  to  --llmm).
              The format for such definitions is
                   _e_v_e_n_t  [--ddeelleettee  _D]  [--rreeppeeaatt  _R]  [--iinnsseerrtt _w]
              [--oobbsseerrvveedd] [--oommiitt]
              The optional flags after the event name modify  the
              default behavior of hidden events in the model.  By
              default events are unobserved pseudo-words of which
              at  most  one  can occur between regular words, and
              which are added to the context to predict following
              words and events.  (A typical use would be to model
              hidden  sentence  boundaries.)   --ddeelleettee  indicates
              that  upon  encountering  the  event,  _D  words are
              deleted from  the  next  word's  context.   --rreeppeeaatt
              indicates  that  after  the  event the next _R words
              from the context are to be repeated.  --iinnsseerrtt spec-
              ifies that an (unobserved) word _w is to be inserted
              into the history.  --oobbsseerrvveedd  specifies  the  event
              tag is not hidden, but observed in the word stream.
              --oommiitt indicates that the event tag itself is not to
              be  added to the history for predicting the follow-
              ing words.
              The hidden event mechanism represents a generaliza-
              tion of the disfluency LM enabled by --ddff.

       --hhiiddddeenn--nnoott
              Modifies processing of hidden event N-grams for the
              case that the event tags are embedded in  the  word
              stream, as opposed to inferred through dynamic pro-
              gramming.

       --ddff    Interpret the LM as containing  disfluency  events.
              This  enables an older form of hidden-event LM used
              in Stolcke & Shriberg (1996).  It is roughly equiv-
              alent to a hidden-event LM with
                   UH -observed -omit       (filled pause)
                   UM -observed -omit       (filled pause)
                   @SDEL -insert <s>        (sentence restart)
                   @DEL1 -delete 1 -omit    (1-word deletion)
                   @DEL2 -delete 2 -omit    (2-word deletion)
                   @REP1 -repeat 1 -omit    (1-word repetition)
                   @REP2 -repeat 2 -omit    (2-word repetition)

       --ccllaasssseess _f_i_l_e
              Interpret  the  LM  as an N-gram over word classes.
              The expansions of the classes are given in _f_i_l_e  in
              ccllaasssseess--ffoorrmmaatt(5).   Tokens  in the LM that are not
              defined as classes in _f_i_l_e are assumed to be  plain
              words,  so  that  the  LM can contain mixed N-grams
              over both words and word classes.
              Class definitions may also follow the N-gram  defi-
              nitions  in  the LM file (the argument to --llmm).  In
              that case --ccllaasssseess //ddeevv//nnuullll should be specified to
              trigger  interpretation  of the LM as a class-based
              model.  Otherwise, class definitions specified with
              this  option  override any definitions found in the
              LM file itself.

       --ssiimmppllee--ccllaasssseess
              Assume a "simple" class model: each word is  member
              of at most one word class, and class expansions are
              exactly one word long.

       --eexxppaanndd--ccllaasssseess _k
              Replace  the  read  class-N-gram  model   with   an
              (approximately)  equivalent word-based N-gram.  The
              argument  _k  limits  the  length  of  the   N-grams
              included  in  the  new model (_k=0 allows N-grams of
              arbitrary length).

       --eexxppaanndd--eexxaacctt _k
              Use a more exact (but also  more  expensive)  algo-
              rithm  to  compute the conditional probabilities of
              N-grams  expanded  from  classes,  for  N-grams  of
              length  _k  or longer (_k=0 is a special case and the
              default, it disables the exact algorithm for all N-
              grams).   The  exact  algorithm  is recommended for
              class-N-gram models that contain  multi-word  class
              expansions,  for N-gram lengths exceeding the order
              of the underlying class N-grams.

       --ddeecciipphheerr
              Use the N-gram model exactly  as  the  Decipher(TM)
              recognizer  would,  i.e., choosing the backoff path
              if it has a  higher  probability  than  the  bigram
              transition,   and  rounding  log  probabilities  to
              bytelog precision.

       --ffaaccttoorreedd
              Use a factored N-gram model,  i.e.,  a  model  that
              represents  words as vectors of feature-value pairs
              and models sequences of words by a  set  of  condi-
              tional dependency relations between factors.  Indi-
              vidual dependencies are modeled by standard  N-gram
              LMs,  allowing  however  for  a generalized backoff
              mechanism to combine multiple backoff paths (Bilmes
              and   Kirchhoff  2003).   The  --llmm,  --mmiixx--llmm,  etc.
              options name FLM specification files in the  format
              described in Kirchhoff et al. (2002).

       --hhmmmm   Use  an  HMM  of  N-grams  language model.  The --llmm
              option specifies a file  that  describes  a  proba-
              bilistic  graph,  with each line corresponding to a
              node or state.  A line has the format:
                   _s_t_a_t_e_n_a_m_e _n_g_r_a_m_-_f_i_l_e _s_1 _p_1 _s_2 _p_2 ...
              where _s_t_a_t_e_n_a_m_e is a string identifying the  state,
              _n_g_r_a_m_-_f_i_l_e names a file containing a backoff N-gram
              model, _s_1,_s_2, ... are names of  follow-states,  and
              _p_1,_p_2, ... are the associated transition probabili-
              ties.  A filename of ``-'' can be used to  indicate
              the  N-gram model data is included in the HMM file,
              after the current line.  (Further HMM states may be
              specified after the N-gram data.)
              The  names  IINNIITTIIAALL  and FFIINNAALL denote the start and
              end states, respectively, and have no associated N-
              gram  model  (_n_g_r_a_m_-_f_i_l_e must be specified as ``.''
              for these).  The --oorrddeerr option specifies the  maxi-
              mal N-gram length in the component models.
              The  semantics  of an HMM of N-grams is as follows:
              as each state is visited, words  are  emitted  from
              the associated N-gram model.  The first state (cor-
              responding to the start-of-sentence) is IINNIITTIIAALL.  A
              state  is  left with the probability of the end-of-
              sentence token in the  respective  model,  and  the
              next state is chosen according to the state transi-
              tion probabilities.  Each  state  has  to  emit  at
              least  one  word.   The  actual  end-of-sentence is
              emitted if and only if the FFIINNAALL state is  reached.
              Each word probability is conditioned on all preced-
              ing words, regardless of whether they were  emitted
              in the same or a previous state.

       --vvooccaabb _f_i_l_e
              Initialize  the  vocabulary  for  the LM from _f_i_l_e.
              This is especially useful if the LM itself does not
              specify a complete vocabulary, e.g., as with --nnuullll.

       --nnoonneevveennttss _f_i_l_e
              Read a list of words from _f_i_l_e that are to be  con-
              sidered non-events, i.e., that should only occur in
              LM contexts, but not as  predictions.   Such  words
              are  excluded  from  sentence generation (--ggeenn) and
              probability summation (--ppppll --ddeebbuugg 33).

       --lliimmiitt--vvooccaabb
              Discard LM parameters on reading that do  not  per-
              tain to the words specified in the vocabulary.  The
              default is that words used in the LM are  automati-
              cally  added to the vocabulary.  This option can be
              used to reduce the memory  requirements  for  large
              LMs  that are going to be evaluated only on a small
              vocabulary subset.

       --uunnkk   Indicates that the LM contains  the  unknown  word,
              i.e., is an open-class LM.

       --mmaapp--uunnkk _w_o_r_d
              Map  out-of-vocabulary  words  to _w_o_r_d, rather than
              the default <<uunnkk>> tag.

       --ttoolloowweerr
              Map all vocabulary to lowercase.   Useful  if  case
              conventions for text/counts and language model dif-
              fer.

       --mmuullttiiwwoorrddss
              Split input words consisting of  multiwords  joined
              by underscores into their components, before evalu-
              ating LM probabilities.

       --mmiixx--llmm _f_i_l_e
              Read a second N-gram model for  interpolation  pur-
              poses.   The second and any additional interpolated
              models can also be class N-grams  (using  the  same
              --ccllaasssseess   definitions),  but  are  otherwise  con-
              strained to be standard N-grams, i.e., the  options
              --ddff, --ttaaggggeedd, --sskkiipp, and --hhiiddddeenn--vvooccaabb do not apply
              to them.
              NNOOTTEE:: Unless --bbaayyeess (see below) is specified, --mmiixx--
              llmm triggers a static interpolation of the models in
              memory.  In most cases a  more  efficient,  dynamic
              interpolation is sufficient, requested by --bbaayyeess 00.
              Also, mixing models of different type (e.g.,  word-
              based  and  class-based)  will  _o_n_l_y work correctly
              with dynamic interpolation.

       --llaammbbddaa _w_e_i_g_h_t
              Set the weight of the main model when interpolating
              with --mmiixx--llmm.  Default value is 0.5.

       --mmiixx--llmm22 _f_i_l_e

       --mmiixx--llmm33 _f_i_l_e

       --mmiixx--llmm44 _f_i_l_e

       --mmiixx--llmm55 _f_i_l_e

       --mmiixx--llmm66 _f_i_l_e

       --mmiixx--llmm77 _f_i_l_e

       --mmiixx--llmm88 _f_i_l_e

       --mmiixx--llmm99 _f_i_l_e
              Up  to  9  more  N-gram models can be specified for
              interpolation.

       --mmiixx--llaammbbddaa22 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa33 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa44 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa55 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa66 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa77 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa88 _w_e_i_g_h_t

       --mmiixx--llaammbbddaa99 _w_e_i_g_h_t
              These are the weights for  the  additional  mixture
              components, corresponding to --mmiixx--llmm22 through --mmiixx--
              llmm99.  The weight for the --mmiixx--llmm model is  1  minus
              the  sum  of --llaammbbddaa and --mmiixx--llaammbbddaa22 through --mmiixx--
              llaammbbddaa99.

       --lloogglliinneeaarr--mmiixx
              Implement a log-linear (rather than linear) mixture
              LM, using the parameters above.

       --bbaayyeess _l_e_n_g_t_h
              Interpolate  the  second  and  the main model using
              posterior probabilities for  local  N-gram-contexts
              of  length  _l_e_n_g_t_h.  The --llaammbbddaa value is used as a
              prior mixture weight in this case.

       --bbaayyeess--ssccaallee _s_c_a_l_e
              Set the exponential scale  factor  on  the  context
              likelihood in conjunction with the --bbaayyeess function.
              Default value is 1.0.

       --ccaacchhee _l_e_n_g_t_h
              Interpolate the main LM (or the one resulting  from
              operations  above)  with  a  unigram cache language
              model based on a history of _l_e_n_g_t_h words.

       --ccaacchhee--llaammbbddaa _w_e_i_g_h_t
              Set interpolation weight for the cache LM.  Default
              value is 0.05.

       --ddyynnaammiicc
              Interpolate  the main LM (or the one resulting from
              operations above) with a dynamically  changing  LM.
              LM  changes  are indicated by the tag ``<LMstate>''
              starting a line in the input to --ppppll,  --ccoouunnttss,  or
              --rreessccoorree, followed by a filename containing the new
              LM.

       --ddyynnaammiicc--llaammbbddaa _w_e_i_g_h_t
              Set  interpolation  weight  for  the  dynamic   LM.
              Default value is 0.05.

       --aaddaapptt--mmaarrggiinnaallss _L_M
              Use   an   LM  obtained  by  adapting  the  unigram
              marginals to the values  specified  in  the  _L_M  in
              nnggrraamm--ffoorrmmaatt(5),  using  the  method  described  in
              Kneser et al. (1997).  The LM to be adapted is that
              constructed according to the other options.

       --bbaassee--mmaarrggiinnaallss _L_M
              Specify  the  baseline unigram marginals in a sepa-
              rate file _L_M, which must be in  nnggrraamm--ffoorrmmaatt(5)  as
              well.  If not specified, the baseline marginals are
              taken from the model to be adapted, but this  might
              not  be  desirable, e.g., when Kneser-Ney smoothing
              was used.

       --aaddaapptt--mmaarrggiinnaallss--bbeettaa _B
              The exponential weight given to the  ratio  between
              adapted  and  baseline  marginals.   The default is
              0.5.

       --aaddaapptt--mmaarrggiinnaallss--rraattiiooss
              Compute and output only the log ratio  between  the
              adapted  and  the baseline LM probabilities.  These
              can be useful as a separate knowledge source in  N-
              best rescoring.

       The  following  options  specify  the operations performed
       on/with the LM constructed as per the options above.

       --rreennoorrmm
              Renormalize the main model by  recomputing  backoff
              weights for the given probabilities.

       --pprruunnee _t_h_r_e_s_h_o_l_d
              Prune  N-gram probabilities if their removal causes
              (training set) perplexity of the model to  increase
              by less than _t_h_r_e_s_h_o_l_d relative.

       --pprruunnee--lloowwpprroobbss
              Prune  N-gram probabilities that are lower than the
              corresponding backed-off estimates.  This generates
              N-gram  models that can be correctly converted into
              probabilistic finite-state networks.

       --mmiinnpprruunnee _n
              Only prune N-grams  of  length  at  least  _n.   The
              default  (and  minimum  allowed  value) is 2, i.e.,
              only unigrams  are  excluded  from  pruning.   This
              option  applies to both --pprruunnee and --pprruunnee--lloowwpprroobbss.

       --rreessccoorree--nnggrraamm _f_i_l_e
              Read an N-gram LM from _f_i_l_e and  recompute  its  N-
              gram  probabilities  using  the LM specified by the
              other options; then renormalize  and  evaluate  the
              resulting new N-gram LM.

       --wwrriittee--llmm _f_i_l_e
              Write  a model back to _f_i_l_e.  The output will be in
              the same format as read by --llmm,  except  if  opera-
              tions  such  as  --mmiixx--llmm  or  --eexxppaanndd--ccllaasssseess  were
              applied, in which case the output will contain  the
              generated  single  N-gram  backoff  model  in  ARPA
              nnggrraamm--ffoorrmmaatt(5).

       --wwrriittee--vvooccaabb _f_i_l_e
              Write the LM's vocabulary to _f_i_l_e.

       --ggeenn _n_u_m_b_e_r
              Generate _n_u_m_b_e_r random sentences from the LM.

       --sseeeedd _v_a_l_u_e
              Initialize the random  number  generator  used  for
              sentence  generation using seed _v_a_l_u_e.  The default
              is to use a seed that should be close to unique for
              each invocation of the program.

       --ppppll _t_e_x_t_f_i_l_e
              Compute  sentence  scores  (log  probabilities) and
              perplexities from the sentences in _t_e_x_t_f_i_l_e,  which
              should  contain  one sentence per line.  The --ddeebbuugg
              option controls the level of detail  printed,  even
              though output is to stdout (not stderr).

              --ddeebbuugg 00  Only  summary  statistics  for the entire
                        corpus are printed,  as  well  a  partial
                        statistics  for each input portion delim-
                        ited  by  escaped  lines  (see  --eessccaappee).
                        These  statistics  include  the number of
                        sentences, words, out-of-vocabulary words
                        and zero-probability tokens in the input,
                        as well as its total log probability  and
                        perplexity.  Perplexity is given with two
                        different  normalizations:  counting  all
                        input tokens (``ppl'') and excluding end-
                        of-sentence tags (``ppl1'').

              --ddeebbuugg 11  Statistics for individual  sentences  are
                        printed.

              --ddeebbuugg 22  Probabilities  for  each  word,  plus LM-
                        dependent  details  about  backoff   used
                        etc., are printed.

              --ddeebbuugg 33  Probabilities for all words are summed in
                        each context, and the sum is printed.  If
                        this  differs  significantly  from  1,  a
                        warning message to stderr will be issued.

       --nnbbeesstt _f_i_l_e
              Read  an  N-best list in nnbbeesstt--ffoorrmmaatt(5) and rerank
              the  hypotheses  using  the  specified   LM.    The
              reordered N-best list is written to stdout.  If the
              N-best list is given in ``NBestList1.0'' format and
              contains  composite acoustic/language model scores,
              then --ddeecciipphheerr--llmm and the recognizer language model
              and  word transition weights (see below) need to be
              specified so the original acoustic  scores  can  be
              recovered.

       --nnbbeesstt--ffiilleess _f_i_l_e_l_i_s_t
              Process  multiple  N-best lists whose filenames are
              listed in _f_i_l_e_l_i_s_t.

       --wwrriittee--nnbbeesstt--ddiirr _d_i_r
              Deposit rescored N-best lists into  directory  _d_i_r,
              using filenames derived from the input ones.

       --ddeecciipphheerr--nnbbeesstt
              Output  rescored  N-best lists in Decipher 1.0 for-
              mat, rather than SRILM format.

       --nnoo--rreeoorrddeerr
              Output rescored N-best lists  without  sorting  the
              hypotheses by their new combined scores.

       --sspplliitt--mmuullttiiwwoorrddss
              Split multiwords into their components when reading
              N-best lists; the rescored  N-best  lists  thus  no
              longer contain multiwords.  (Note this is different
              from the --mmuullttiiwwoorrddss option, which leaves the input
              word  stream  unchanged  and splits multiwords only
              for the purpose of LM probability computation.)

       --mmaaxx--nnbbeesstt _n
              Limits the number of hypotheses read from an N-best
              list.  Only the first _n hypotheses are processed.

       --rreessccoorree _f_i_l_e
              Similar  to --nnbbeesstt, but the input is processed as a
              stream of N-best hypotheses (without header).   The
              output consists of the rescored hypotheses in SRILM
              format (the  third  of  the  formats  described  in
              nnbbeesstt--ffoorrmmaatt(5)).

       --ddeecciipphheerr--llmm _m_o_d_e_l_-_f_i_l_e
              Designates  the  N-gram  backoff model (typically a
              bigram) that was used by  the  Decipher(TM)  recog-
              nizer   in   computing  composite  scores  for  the
              hypotheses fed to --rreessccoorree or --nnbbeesstt.  Used to com-
              pute acoustic scores from the composite scores.

       --ddeecciipphheerr--oorrddeerr _N
              Specifies  the  order  of the Decipher N-gram model
              used (default is 2).

       --ddeecciipphheerr--nnoobbaacckkooffff
              Indicates that the Decipher N-gram model  does  not
              contain  backoff  nodes,  i.e.,  all  recognizer LM
              scores are correct up to rounding.

       --ddeecciipphheerr--llmmww _w_e_i_g_h_t
              Specifies the language model  weight  used  by  the
              recognizer.   Used  to compute acoustic scores from
              the composite scores.

       --ddeecciipphheerr--wwttww _w_e_i_g_h_t
              Specifies the word transition weight  used  by  the
              recognizer.   Used  to compute acoustic scores from
              the composite scores.

       --eessccaappee _s_t_r_i_n_g
              Set an ``escape string'' for the --ppppll, --ccoouunnttss, and
              --rreessccoorree  computations.   Input lines starting with
              _s_t_r_i_n_g are not processed as  sentences  and  passed
              unchanged  to  stdout instead.  This allows associ-
              ated information to be passed  to  scoring  scripts
              etc.

       --ccoouunnttss _c_o_u_n_t_s_f_i_l_e
              Perform  a  computation  similar to --ppppll, but based
              only on the  N-gram  counts  found  in  _c_o_u_n_t_s_f_i_l_e.
              Probabilities  are  computed  for  the last word of
              each N-gram, using the other words as contexts, and
              scaling  by  the  associated N-gram count.  Summary
              statistics are output at the end, as well as before
              each escaped input line.

       --ccoouunntt--oorrddeerr _n
              Use  only counts of order _n in the --ccoouunnttss computa-
              tion.  The default value  is  0,  meaning  use  all
              counts.

       --ccoouunnttss--eennttrrooppyy
              Weight the log probabilities for --ccoouunnttss processing
              by the join probabilities  of  the  N-grams.   This
              effectively   computes  the  sum  over  p(w,h)  log
              p(w|h), i.e., the entropy of the model.  In  debug-
              ging  mode,  both the conditional log probabilities
              and the corresponding joint probabilities are  out-
              put.

       --sskkiippoooovvss
              Instruct  the LM to skip over contexts that contain
              out-of-vocabulary words, instead of using a backoff
              strategy in these cases.

       --nnooiissee _n_o_i_s_e_-_t_a_g
              Designate _n_o_i_s_e_-_t_a_g as a vocabulary item that is to
              be ignored by the LM.  (This is typically  used  to
              identify  a noise marker.)  Note that the LM speci-
              fied by --ddeecciipphheerr--llmm does NOT ignore this _n_o_i_s_e_-_t_a_g
              since  the  DECIPHER  recognizer  treats noise as a
              regular word.

       --nnooiissee--vvooccaabb _f_i_l_e
              Read several noise tags from _f_i_l_e, instead  of,  or
              in  addition  to, the single noise tag specified by
              --nnooiissee.

       --rreevveerrssee
              Reverse the words in a sentence for LM scoring pur-
              poses.   (This assumes the LM used is a ``right-to-
              left'' model.)   Note  that  the  LM  specified  by
              --ddeecciipphheerr--llmm  is  always  applied  to the original,
              left-to-right word sequence.

SSEEEE AALLSSOO
       ngram-count(1),   ngram-class(1),   lm-scripts(1),    ppl-
       scripts(1),  pfsg-scripts(1), nbest-scripts(1), ngram-for-
       mat(5), nbest-format(5), classes-format(5).
       J. A. Bilmes and K. Kirchhoff, ``Factored Language  Models
       and  Generalized  Parallel Backoff,'' _P_r_o_c_. _H_L_T_-_N_A_A_C_L, pp.
       4-6, Edmonton, Alberta, 2003.
       K. Kirchhoff et al., ``Novel Speech Recognition Models for
       Arabic,''  Johns  Hopkins University Summer Research Work-
       shop 2002, Final Report.
       R. Kneser, J. Peters and D. Klakow, ``Language Model Adap-
       tation  Using  Dynamic  Marginals'', _P_r_o_c_. _E_u_r_o_s_p_e_e_c_h, pp.
       1971-1974, Rhodes, 1997.
       A. Stolcke and E. Shriberg, ``Statistical language  model-
       ing  for  speech  disfluencies,''  Proc.  IEEE ICASSP, pp.
       405-409, Atlanta, GA, 1996.
       A. Stolcke,`` Entropy-based Pruning  of  Backoff  Language
       Models,''  _P_r_o_c_.  _D_A_R_P_A  _B_r_o_a_d_c_a_s_t  _N_e_w_s _T_r_a_n_s_c_r_i_p_t_i_o_n _a_n_d
       _U_n_d_e_r_s_t_a_n_d_i_n_g _W_o_r_k_s_h_o_p, pp. 270-274, Lansdowne, VA,  1998.
       A.  Stolcke  et  al.,  ``Automatic  Detection  of Sentence
       Boundaries and Disfluencies based on  Recognized  Words,''
       _P_r_o_c_. _I_C_S_L_P, pp. 2247-2250, Sydney, 1998.
       M. Weintraub et al., ``Fast Training and Portability,'' in
       Research Note No. 1, Center for Language and  Speech  Pro-
       cessing, Johns Hopkins University, Baltimore, Feb. 1996.

BBUUGGSS
       Some  LM  types  (such  as Bayes-interpolated and factored
       LMs) currently do not support the --wwrriittee--llmm function.

       For the --lliimmiitt--vvooccaabb option to work correctly with  hidden
       event  and  class N-gram LMs, the event/class vocabularies
       have  to  be  specified  by  options  (--hhiiddddeenn--vvooccaabb   and
       --ccllaasssseess,  respectively).   Embedding  event/class defini-
       tions in the LM file only will not work correctly.

       Sentence generation is slow and takes time proportional to
       the vocabulary size.

       The  file  given  by  --ccllaasssseess  is  read multiple times if
       --lliimmiitt--vvooccaabb is in effect or if a mixture of LMs if speci-
       fied.   This  will lead to incorrect behavior if the argu-
       ment of --ccllaasssseess is stdin (``-'').

       Also, --lliimmiitt--vvooccaabb will not work correctly with LM  opera-
       tions that require the entire vocabulary to be enumerated,
       such as --aaddaapptt--mmaarrggiinnaallss or  perplexity  computation  with
       --ddeebbuugg 33.

       Support for factored LMs is experimental and many LM oper-
       ations supported by  standard  N-grams  (such  as  --lliimmiitt--
       vvooccaabb) are not implemented yet.

AAUUTTHHOORR
       Andreas Stolcke <stolcke@speech.sri.com>.
       Copyright 1995-2005 SRI International



SRILM Tools        $Date: 2005/07/23 01:45:43 $          ngram(1)
